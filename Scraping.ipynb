{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "#initiate data storage\n",
    "Date = []\n",
    "Author = []\n",
    "Title = []\n",
    "Articles = []\n",
    "PDFlink = []\n",
    "df = pd.DataFrame(columns=[\"Date\",\"Author\",\"Title\",\"Articles\"])\n",
    "href='<a href=\"publications/news/774001_22092020-1.pdf\"'\n",
    "start = href.find('<a href=\"')\n",
    "for page in range(1,5):\n",
    "        r = requests.get(\"http://www.bnacapitaux.com.tn/?pg=\"+str(page)+\"&news=all\")\n",
    "        soup =   BeautifulSoup(r.content,\"lxml\")\n",
    "        #Extract date\n",
    "        for dateDiv in soup.find_all(class_=\"NewsDate\")[:6]:\n",
    "            match = re.search(r'\\d{2}/\\d{2}/\\d{4}', dateDiv.text.replace(\"\\n\",\"\"))\n",
    "            datetime = parser.parse(match.group())\n",
    "            Date.append(datetime)\n",
    "        \n",
    "        #Extract author\n",
    "        for author in soup.find_all(class_=\"NewsDate\")[:6]:\n",
    "            \n",
    "                Author.append(author.text.replace(\"\\n\",\"\").strip()[25:-1])\n",
    "            \n",
    "                \n",
    "        #Extract title       \n",
    "        for title in soup.find_all(class_=\"NewsTitle\")[:6]:\n",
    "            Title.append(title.text.replace(\"\\n\",\"\").strip())\n",
    "            \n",
    "        #Extract description    \n",
    "        for article in soup.find_all('div', id=lambda x: x and x.startswith('NEW_DESC'))[:6]:\n",
    "            \n",
    "                Articles.append(article.text.replace(\"\\n\",\"\").strip())\n",
    "                \n",
    "        #Extract PDFlink        \n",
    "        liste=[]\n",
    "        liste1=[]\n",
    "        for link in soup.find_all('a', href=lambda x: x and x.startswith('publications')):\n",
    "            match = re.search(r'\\bAttachement\\b', str(link))\n",
    "            if match !=None :\n",
    "                liste.append(str(link)[9:60])\n",
    "                for l in liste:\n",
    "                    pos= l.find('\"')\n",
    "                    l=(\"http://www.bnacapitaux.com.tn/\"+l[0:pos])\n",
    "                liste1.append(l)     \n",
    "        PDFlink=PDFlink+liste1[0:6]\n",
    "\n",
    "#Fill the dataframe\n",
    "for i in range(len(Date)):\n",
    "    df = df.append({'Date':Date[i],'Author':Author[i],'Title':Title[i],'Articles':Articles[i],'PDF':PDFlink[i]},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.bnacapitaux.com.tn/publications/news/763001_20102020-1.pdf'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PDF'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel(r'./ArticlesBNAcapitaux.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Using cached bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "Collecting soupsieve>1.2; python_version >= \"3.0\"\n",
      "  Using cached soupsieve-2.0.1-py3-none-any.whl (32 kB)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1279 sha256=67436ac005e0a6615ffde1968eddfa54ea5224f5feebaeb176d83e23ab657761\n",
      "  Stored in directory: c:\\users\\amin\\appdata\\local\\pip\\cache\\wheels\\0a\\9e\\ba\\20e5bbc1afef3a491f0b3bb74d508f99403aabe76eda2167ca\n",
      "Successfully built bs4\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.9.3 bs4-0.0.1 soupsieve-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
